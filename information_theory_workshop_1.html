<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Introduction to Epidemiology Applications of Information Theory I</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Information Theory</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="Intro_R.html">Intro to R</a>
</li>
<li>
  <a href="information_theory_workshop_1.html">Information Theory Basics</a>
</li>
<li>
  <a href="information_theory_workshop_2.html">Bayesian Probability</a>
</li>
<li>
  <a href="information_theory_workshop_3.html">Metrics</a>
</li>
<li>
  <a href="sdm_spores.pdf">SDM Spores</a>
</li>
<li>
  <a href="SDM_Pseudo_R2.html">SDM Predictivity</a>
</li>
<li>
  <a href="about.html">Further Readings</a>
</li>
<li>
  <a href="responses.html">Responses</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Introduction to Epidemiology Applications of Information Theory I</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This short workshop is focused on some introductory concepts from the field of Information Theory that can be used in epidemiology, particularly in the analysis of disease forecasters and risk prediction systems. The use of information theory in plant pathology has a relatively short history. McRoberts <em>et al.</em> suggested that expected information — commonly referred to as the <em>entropy</em> or <em>information entropy</em> — was a potentially useful concept to understand individual-level differences in response to the output from disease forecasting systems. Most of the work in developing concepts from Information Theory for use in plant pathology has been done by Gareth Hughes and sumamrized in a short monograph, Hughes (2012) <em>Applications of Information Thoery to Epidemiology</em> and a series of papers with several collaborators (see the <em>Further Readings</em> section for more information).</p>
</div>
<div id="definite-messages---shannon-and-quantifying-information" class="section level2">
<h2>Definite Messages - Shannon and quantifying information</h2>
<p>All of the information theoretic concepts we will meet in this workshop can be traced directly to the pioneering work of Claude Shannon, whose “<em>A Mathematical Theory of Communication</em>” (Shannon, 1948) laid out an axiomatic definition of the information content of uncertain messages and then developed many useful results that follow from the initial definition. The most important result from this work is that the information content of a definite message about an uncertain event is given by the logarithm of the reciprocal of the probability of the event, or equivalently minus the logarithm of the probability. To make that definition more concrete we give the definition in mathematical form, adopting the notation of Hughes (2012). Suppose a crop may be in one of <em>m</em> states, <span class="math inline">\(D_{1}, D_{2}, \dots D_{m}\)</span>, which occur with probabilities <span class="math inline">\(Pr(D_{1}), Pr(D_{2}) \dots Pr(D_{m})\)</span>. Now, assuming that the states <span class="math inline">\(D_{j}, j=1 \dots m\)</span> are exhaustive and mutually exclusive, the two constraints <span class="math inline">\(\sum_{j=1}^{m} Pr(D_{j})=1\)</span> and <span class="math inline">\(Pr(D_{j} \ge 0\)</span> allow us to define the information content of a definite message that the crop is in state, <span class="math inline">\(D_{j}\)</span> as:<span class="math display">\[
h(D_{j})=log\left(\frac{1}{Pr(D_{j})}\right)=log(1)-log(Pr(D_{j}))=-log(Pr(D_{j}))\]</span></p>
<p>The applied aspects of Shannons work were mainly concerned with coding messages for transferring information on error-prone communication systems, and with compressing messages to allow more efficient transfer of large quantities of data. Shannon’s work highlighted the fundamental connection between information theory and coding theory. We will not be pursuing those connections during this workshop, but, we note in passing the important result from coding theory known as the Kraft (or Kraft-McMillan) inequality, which draws a formal correspondence between probability distributions and codelength functions. We give the technical result here without much interpretation leaving it to interested readers to pursue the topic at their leisure. Suppose we have a set of symbols <span class="math inline">\(s_{1}, s_{2}, \dots s_{n}\)</span> in an alphabet of size, <span class="math inline">\(r\)</span>. We encode the symbols using strings of bits according to some suitable code, resulting in codewords of lengths <span class="math inline">\(l_{1}, l_{2}, \dots l_{n}\)</span>, then, the Kraft-McMillan inequality states that:<span class="math display">\[
\sum_{i=1}^{n}r^{-l_{i}} \le 1\]</span></p>
<p>Suppose we were assigned the task of writing a code to describe a set of symbols from this alphabet, we are free to assign whatever codeword we want to each symbol and all we know in advance is the probability with which each symbol will occur. If the code is to be as short as possible it makes sense to assign the longest codelengths to the symbols with lowest probability of occurring, and the shortest codelengths to the symbols occur with the highest probabilities, and the Kraft-McMillan inequality suggests that one approach is to assign the codewords such that <span class="math inline">\(l_{i}=-log(Pr(s_{i}))\)</span>. You will notice that the right hand side of the last equation is the same as just the definition of the information content of a definite message, as defined by Shannon.</p>
<p>Returning to thinking about the information content of definite messages, the following code snippet shows the relationship in a simple graph.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
h_p&lt;-<span class="op">-</span><span class="kw">log</span>(p,<span class="dv">2</span>)
<span class="kw">plot</span>(p,h_p, <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;probabliity&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Information (bits)&quot;</span>)</code></pre></div>
<p><img src="information_theory_workshop_1_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Note that the base of the logarithm was specified as 2, which results in the information being calculated in bits. The choice of base for the logarithms only affects tnumberical results, not the algebraic ones; depending on context it is common to see either base 2 or base <span class="math inline">\(e\)</span> (natural logarithms) used. Remember it is straightforward to translate quantities expressed in one base to another base using the change-of-base expression:<span class="math display">\[
log_{a}(x) = \frac{log_{b}(x)}{log_{b}(a)}\]</span></p>
<p>Returning to the information itself, a coupel of standard observations are worth repeating. When the probability is 1 — and thus the event associated with the probability is certain — the information content in a definite message that the event has occurred is zero. As the probability of an event decreases, the information content in a definite message that the event has occurred increases. When the probability of the event is zero — and thus the event is impossible — the information content in a definite message that the event has occurred is undefined.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(p,h_p, <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;probabliity&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Information (bits)&quot;</span>)
  <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="fl">0.5</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="information_theory_workshop_1_files/figure-html/unnamed-chunk-2-1.png" width="672" /> In the plot above we simply added horizontal lines at values of <span class="math inline">\(p=0.5\)</span> and <span class="math inline">\(h(p)=1\)</span> using the <code>abline()</code> function which applies its arguments to the immediately preceding call to the <code>plot()</code> function. Note that the lines cross on the information quantity line; measured in bits, the information content of a definite message that an event with probability <span class="math inline">\(p=0.5\)</span> has occurred (or not) is one bit.</p>
<p>To generate the plots we specified <span class="math inline">\(p\)</span> as a sequence from 0.01 to 1 in steps of 0.01. As we have already noted the information content of definite messages about rare events rises steeply as the probability approaches zero. The next plot shows a little more detail at the lower end of the probability scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">small_p&lt;-<span class="kw">seq</span>(<span class="fl">0.001</span>,<span class="fl">0.01</span>,<span class="fl">0.0001</span>)
h_s_p2&lt;-<span class="st"> </span><span class="op">-</span><span class="kw">log</span>(small_p,<span class="dv">2</span>) <span class="co">#bits</span>
h_s_pe&lt;-<span class="st"> </span><span class="op">-</span><span class="kw">log</span>(small_p) <span class="co">#default natural logarithm - nats</span>
h_s_p10&lt;-<span class="st"> </span><span class="op">-</span><span class="kw">log10</span>(small_p) <span class="co">#Hartleys</span>
<span class="kw">plot</span>(small_p,h_s_p2, <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;probability&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;information quantities&quot;</span>,
     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">10</span>))
  <span class="kw">lines</span>(small_p, h_s_pe,<span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)
  <span class="kw">lines</span>(small_p, h_s_p10,<span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)
  <span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;bits&quot;</span>,<span class="st">&quot;nats&quot;</span>,<span class="st">&quot;Hartleys&quot;</span>),<span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>),
         <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>))</code></pre></div>
<p><img src="information_theory_workshop_1_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>It is obvious in the preceding figure that the different units of information show the same relationship with probability. We leave it to you to use the change-of-base expression to show that they are equivalent.</p>
</div>
<div id="indefinite-messages-entropyexpected-information" class="section level2">
<h2>Indefinite messages — entropy/expected information</h2>
<p>The results in the previous section applied to definite messages (or certain knowledge) of events that have different potential outcomes. In the context of disease forecasting, and often with diagnostics, we are not in the position of having definite messages about the event(s) of interest. In such situations — where we have indefinite messages about uncertain events — until we know the outcome, we can calculate the <em>expected information content</em>, or <em>entropy</em> of the message. As the name implies this information quantity is an expected value, and as is typical of expected values it is a probability weighted average of the different possible values; what is unusal about it is that the values to be weighted are the logarithms of the probabilities themselves.</p>
<p>Before considering the general formula for the <em>expected information content</em> we will show the formula for a binary event written out as an explicit summation; as before we assume the event of interest can occur with probability, <span class="math inline">\(p\)</span>. In that case the <em>expected information content</em> of messages concerning the event is given by<span class="math display">\[
H(p)=-\left(p \times log(p)\right)+\left((1-p) \times log(1-p)\right)
\]</span> Generalizing the summation over <span class="math inline">\(i\)</span> values of <span class="math inline">\(p_{i}\)</span> gives Shannon’s famous formula for expected information content (or &quot; <em>entropy</em>“):<span class="math display">\[
H(p_{i})=-\sum_{i}p_{i}log \left(p_{i} \right)\]</span></p>
<p>For a binary event the relationship between the probability of the event occurring and <span class="math inline">\(H(p)\)</span> is as shown in the following figure (with <span class="math inline">\(H(p)\)</span> calculated in bits).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">compl_p &lt;-<span class="dv">1</span><span class="op">-</span>p
log2p &lt;-<span class="st"> </span><span class="kw">log</span>(p,<span class="dv">2</span>)
log2_compl_p &lt;-<span class="kw">log</span>(compl_p,<span class="dv">2</span>)
sum_terms &lt;-(p<span class="op">*</span>log2p)<span class="op">+</span>(compl_p<span class="op">*</span>log2_compl_p)
H_p &lt;-<span class="st"> </span><span class="op">-</span>sum_terms
<span class="kw">plot</span>(p,H_p, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">ylab=</span><span class="st">&quot;Expected information (bits)&quot;</span>)</code></pre></div>
<p><img src="information_theory_workshop_1_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
